<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PIGINet: A Transformer-based Plan Feasibility Predictor for Faster Task and Motion Planning">
  <meta name="keywords" content="Transformers, Task and Motion Planning, Manipulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PIGINet</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateResultVideo();">

<section class="hero" style="background-color: #ffffff; z-index:-10">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PIGINet: A Transformer-based Plan Feasibility Predictor for Robotic Rearrangement in Geometrically Complex Environments</h1>
          <!-- <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.robot-learning.org/">CoRL 2022</a></h3> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://zt-yang.com/">Zhutian Yang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="http://web.mit.edu/caelan/www/">Caelan Garrett</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="http://people.csail.mit.edu/tlp/">Tomás Lozano-Pérez</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="http://people.csail.mit.edu/lpk/">Leslie Kaelbling</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a><sup>1, 2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>MIT,</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="paper/RSS_PIGINet.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://www.youtube.com/watch?v=kDliSCMW2XY"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/Learning-and-Intelligent-Systems/kitchen-worlds"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            </div>
          </div>


        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd" style="margin-bottom: -300px; z-index:-200">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="300px">
            <source src="media/kitchens.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero" style="background-color: #ffffff; z-index:-10">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <h2 class="subtitle has-text-centered">
        <span class="dpiginet">PIGINet</span> predicts the feasibility of a task <span class="dpiginet">P</span>lan given <span class="dpiginet">I</span>mages of objects, <span class="dpiginet">G</span>oal description, and <span class="dpiginet">I</span>nitial state descriptions. <br>It reduces the planning time of a task and motion planner by 50-80% by eliminating infeasible task plans.</b>
        </h2>
      </div>

      <img src="media/figures/io.png" class="full-image" style="width:50%"
         alt="Input and output of PIGINet plan feasibility predictor" />

    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">

      <h3 class="title is-3 has-text-centered" style="padding-bottom: 20pt;">Motivations</h3>

      <h2 class="subtitle has-text-centered">
      LLMs are good at generating high-level plans, but pick-and-place actions may be infeasible when there are articulated or movable obstacles.
      </h2>

      <div id="results-carousel" class="carousel results-carousel">

        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/collisions1_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/collisions2_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/collisions3_1.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/collisions1_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/collisions2_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/collisions3_1.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">

      <h2 class="subtitle has-text-centered">
      <span class="dpiginet">PIGINet</span> uses images to decide which sequence of actions are feasible in rearrangement tasks involving storage space and cluttered surface.
      </h2>

      <div id="results-carousel" class="carousel results-carousel">

        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/tasks1_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/tasks2_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/tasks3_1.mp4"
                    type="video/mp4">
          </video>
        </div>

        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/tasks1_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/tasks2_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/tasks3_1.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>
  </div>
</section>

<section class="hero" style="background-color: #ffffff;">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <h2 class="subtitle has-text-centered">
        <span class="dpiginet">PIGINet</span> is a Transformer-based architecture that fuses features of images, text, and values describing the problem and plan.<br>
        </h2>
      </div>

      <img src="media/figures/arch.png" class="full-image" style="width:50%"
         alt="Architecture of PIGINet plan feasibility predictor" />

    </div>
  </div>

  <div class="container is-max-widescreen">

    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h3 class="title is-3 has-text-centered">Abstract</h3>

        <p class="justify">
            <img src="media/figures/images.jpg" class="interpolation-image" width="480" align="right"
                 style="margin:0% 1% "
                 alt="Image views" />
             We present a learning-enabled Task and Motion Planning (TAMP) algorithm for solving mobile manipulation problems in environments with many articulated and movable obstacles. Our idea is to bias the search procedure of a traditional TAMP planner with a learned plan feasibility predictor. <br><br>The core of our algorithm is PIGINet, a novel Transformer-based learning method that takes in a task plan, the goal, and the initial state, and predicts the probability of finding motion trajectories associated with the task plan. The elements of each action or relation in the initial state–such as text, object poses, and door joint angles– are processed to produce embeddings of the same dimension and fused together to produce each token in the input sequence to transformer encoder. A pre-trained <a href="https://openai.com/blog/clip/">CLIP</a> model is used to generate corresponding text and image embeddings. <br><br>We integrate PIGINet within a TAMP planner that generates a diverse set of high-level task plans, sorts them by their predicted likelihood of feasibility, and refines them in that order. We evaluate the runtime of our TAMP algorithm on seven families of kitchen rearrangement problems, comparing its performance to that of non-learning baselines. Our experiments show that PIGINet substantially improves planning efficiency, cutting down runtime by 80% on problems with small state spaces and 10%-50% on larger ones, after being trained on only 150-600 problems. Finally, it also achieves zero-shot generalization to problems with unseen object categories thanks to its visual encoding of objects.
          </p>
        <br><br>

      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered has-text-centered">
        <div class="column is-two-thirds">
          <h3 class="title is-3 has-text-centered">Video</h3>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/kDliSCMW2XY?rel=0&amp;showinfo=0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body has-text-centered">
    <div class="container">

      <h3 class="title is-3">Results<br></h3>

      Choose a problem:
      <div class="select is-small is-rounded">
        <select id="single-menu-replay" onchange="updateResultVideo()">
          <option value="food-in-cabinet" selected="selected">"all food items are in the upper cabinet"</option>
          <!-- <option value="food-in-cabinet-door">"all food items are in the upper cabinet (one of the cabinet door is open)"</option> -->
          <option value="food-in-fridge">"all food items are in the fridge"</option>
          <option value="food-in-fridge-door">"all food items are in the fridge (one of the fridge door is open)"</option>
          <option value="food-in-fridge-pot">"all food items are in the fridge (the pot contains one food item)"</option>
          <option value="zucchini-in-pot">"the zucchini is in the pot"</option>
          <option value="potato-in-pot">"the potato is in the pot (pot is occupied)"</option>
          <option value="zucchini-in-sink">"the zucchini is in the sink (small sink space)"</option>
          <option value="potato-in-sink-harder">"the potato is in the sink (smaller sink space)"</option>
          <option value="sweetpotato-in-sink">"the sweet potato is in the sink (bigger sink space)"</option>
        </select>
      </div><br>

      <video id="result-video" class="full-image" muted autoplay loop width="70%">
        <source src="media/results/food-in-cabinet.mp4"
                type="video/mp4">
      </video><br>

      <div id="result-table" w3-include-html="result-food-in-cabinet.html"></div><br>

      <div id="table-note">
        Note: <br>
        <ul>
          <li>In the video, furnitures that block the top-down view of the robot arm are removed (upper panel), while the side view shows all furnitures (lower panel).</li>
           <li>In each table, the cell with green background indicates the task plan successfully refined and visualized in the video. PIGINet speeds up planning by reducing the task plans attempted before refining that task plan.</li>
           <li>The arguments to actions are object instances, e.g. tomato, tomato#1</li>
           <li>Move the cursor to the object names in bold, the corresponding object instance will be displayed. Note that those segmented images are rendered in full visibility with transparent background for better understanding; they are not the segmented images used by PIGINet.</li>
        </ul>
      </div>

    </div>
  </div>

</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a> and <a href="https://mohitshridhar.com/">Mohit Shridhar</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


<!-- <script>
includeHTML();
</script> -->

</body>
</html>
